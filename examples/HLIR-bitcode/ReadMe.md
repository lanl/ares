HLIR Bitcode
============

In its current form, HLIR is represented as a canonical set of metadata applied
to LLVM IR. This derectory contains two files, `test.c` and `test.ll`. `test.ll`
was generated by `test.c`, but hand modified to make to code execute in
parallel. The code will add up the results of three instances of the Ackerman
function.

Constructs
----------
Let's look at the modifications made...

```llvm
; Function Attrs: nounwind uwtable
define i32 @main(i32 %argc, i8** %argv) #0 {
entry:
  %x = call i32 @ack(i32 3, i32 3), !hlir.task !2
  %y = call i32 @ack(i32 4, i32 1), !hlir.task !2
  %z = call i32 @ack(i32 4, i32 1), !hlir.task !2
  %add = add nsw i32 %x, %y
  %add2 = add nsw i32 %add, %z
  %call4 = call i32 (i8*, ...) @printf(i8* getelementptr inbounds ([18 x i8], [18 x i8]* @.str, i32 0, i32 0), i32 %add2)
  ret i32 0
}
```

Notice the `!hlir.task` metadata attached to each call? This is how HLIR
represents tasks being launched. Another thing worth noting is that this
presents no concept of a future --- the results of the tasks are simply used as
any normal value is used.

Compiling and Lowering
----------------------
If test.ll were to be compiled as is, the result would be a serial
implementation of the code. The metadata would simply be ignored, and the code
would be generated as usual. To see this, simply run `clang test.ll`, and run
the code.

If we would like to see how to lower this HLIR code into a plain LLVM IR using
pthreads, we can run a code transform that will lower the HLIR into an
LLIR. Using the `opt` tool in the install directory of the project, run
```
opt -S -load <install-dir>/lib/LLVMHLIR.so -hlir.pthread < test.ll > out.ll
```
where `<install-dir>` is the path to the install directory for the project. This
will generate a new file `out.ll`, containing transformed code. Assuming the
universe isn't against us, we should see some output like so...
```llvm
; Function Attrs: nounwind uwtable
define i32 @main(i32 %argc, i8** %argv) #0 {
entry:
  %0 = alloca i64
  %1 = alloca %0
  %2 = getelementptr %0, %0* %1, i64 0, i32 1
  store i32 3, i32* %2
  %3 = getelementptr %0, %0* %1, i64 0, i32 2
  store i32 3, i32* %3
  %4 = bitcast %0* %1 to i8*
  %5 = call i32 @pthread_create(i64* %0, %union.pthread_attr_t* null, i8* (i8*)* @hlir.pthread.wrapped.ack, i8* %4)
  %6 = alloca i64
  %7 = alloca %0
  %8 = getelementptr %0, %0* %7, i64 0, i32 1
  store i32 4, i32* %8
  %9 = getelementptr %0, %0* %7, i64 0, i32 2
  store i32 1, i32* %9
  %10 = bitcast %0* %7 to i8*
  %11 = call i32 @pthread_create(i64* %6, %union.pthread_attr_t* null, i8* (i8*)* @hlir.pthread.wrapped.ack, i8* %10)
  %12 = alloca i64
  %13 = alloca %0
  %14 = getelementptr %0, %0* %13, i64 0, i32 1
  store i32 4, i32* %14
  %15 = getelementptr %0, %0* %13, i64 0, i32 2
  store i32 1, i32* %15
  %16 = bitcast %0* %13 to i8*
  %17 = call i32 @pthread_create(i64* %12, %union.pthread_attr_t* null, i8* (i8*)* @hlir.pthread.wrapped.ack, i8* %16)
  %18 = load i64, i64* %0
  %19 = call i32 @pthread_join(i64 %18, i8** null)
  %20 = getelementptr %0, %0* %1, i64 0, i32 0
  %21 = load i32, i32* %20
  %22 = load i64, i64* %6
  %23 = call i32 @pthread_join(i64 %22, i8** null)
  %24 = getelementptr %0, %0* %7, i64 0, i32 0
  %25 = load i32, i32* %24
  %add = add nsw i32 %21, %25
  %26 = load i64, i64* %12
  %27 = call i32 @pthread_join(i64 %26, i8** null)
  %28 = getelementptr %0, %0* %13, i64 0, i32 0
  %29 = load i32, i32* %28
  %add2 = add nsw i32 %add, %29
  %call4 = call i32 (i8*, ...) @printf(i8* getelementptr inbounds ([18 x i8], [18 x i8]* @.str, i32 0, i32 0), i32 %add2)
  ret i32 0
}
```
This very long winded code (hopefully will be made smaller when the runtime
library is added) now has three calls to `pthread_create` and `pthread_join`. A
structure as been made for every launched call to pass the arguments with, as
well as store the future result. Note how the `pthread_join` calls are put off
as far as possible. This is by designe (though is currently limited to only
within a single block, for prototype reasons).

To now compile the parallel version, we run `clang -lpthread out.ll`. This code
will generate the same result as above, but now running each call to the
Ackermann function on a different thread.
